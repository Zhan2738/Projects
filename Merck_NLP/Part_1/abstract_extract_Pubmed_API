import csv
import re
import urllib.request
from time import sleep

# user inputs what you want to search pubmed for
query = input ("What would you like to download PubMed abstracts for? Enter your keyword(s):")

# if spaces were entered, replace them with %20 to make compatible with PubMed search
query = query.replace(" ", "%20")

# common settings between esearch and efetch
base_url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/'
db = 'db=pubmed'

# esearch settings
search_eutil = 'esearch.fcgi?'
search_term = '&term=' + query + '+journal%20article[pt]'
search_retmax= '&retmax=100'
search_usehistory = '&usehistory=y'
search_rettype = '&rettype=json'

# call the esearch command for the query and read the web result
search_url = base_url+search_eutil+db+search_term+search_retmax+search_usehistory+search_rettype
print("this is the esearch command:\n" + search_url + "\n")
f = urllib.request.urlopen (search_url)
search_data = f.read().decode('utf-8')

# extract the total abstract count
total_abstract_count = int(re.findall("<Count>(\d+?)</Count>",search_data)[0])
final_abstract_number = int(re.findall("<RetMax>(\d+?)</RetMax>",search_data)[0])
print(total_abstract_count)
print(final_abstract_number)

# efetch settings
fetch_eutil = 'efetch.fcgi?'
fetch_retmode = "&retmode=text"
fetch_rettype = "&rettype=abstract"

# obtain webenv and querykey settings from the esearch results
fetch_webenv = "&WebEnv=" + re.findall ("<WebEnv>(\S+)<\/WebEnv>", search_data)[0]
fetch_querykey = "&query_key=" + re.findall("<QueryKey>(\d+?)</QueryKey>",search_data)[0]


# call efetch commands using a loop until all abstracts are obtained
all_abstracts = list()

fetch_retmax = "&retmax=" + str(final_abstract_number)
# create the efetch url
fetch_url = base_url+fetch_eutil+db+fetch_querykey+fetch_webenv+fetch_retmax+fetch_retmode+fetch_rettype
print(fetch_url)
# open the efetch url
f = urllib.request.urlopen (fetch_url)
fetch_data = f.read().decode('utf-8')
# split the data into individual abstracts
abstracts = fetch_data.split("\n\n\n")
# append to the list all_abstracts
all_abstracts = all_abstracts+abstracts

# write all_abstracts to a csv file for downstream data analysis
with open("abstracts.csv", "wt",encoding='utf-8') as abstracts_file:
    # csv writer for full abstracts
    abstract_writer = csv.writer(abstracts_file)
    #abstract_writer.writerow(['Journal', 'Title', 'Authors', 'Author_Information', 'Abstract', 'DOI', 'Misc'])
    #For each abstract, split into categories and write it to the csv file
    for abstract in all_abstracts:
        #To obtain categories, split every double newline.
        split_abstract = abstract.split("\n\n")
        #only get the abstract
        print(split_abstract[4])
        if len(split_abstract) > 5:
            abstract_writer.writerow([split_abstract[4]])
    
    #transvert csv into json format
import csv 
import json 

def csv_to_json(csvFilePath, jsonFilePath):
    jsonArray = []
      
    #read csv file
    with open(csvFilePath, encoding='utf-8') as csvf: 
        #load csv file data using csv library's dictionary reader
        csvReader = csv.DictReader(csvf) 

        #convert each csv row into python dict
        for row in csvReader: 
            #add this python dict to json array
            jsonArray.append(row)
  
    #convert python jsonArray to JSON String and write to file
    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: 
        jsonString = json.dumps(jsonArray, indent=4)
        jsonf.write(jsonString)
          
csvFilePath = "C:\\Users\\qian zhang\\Desktop\\abstracts.csv"
jsonFilePath = "C:\\Users\\qian zhang\\Desktop\\abstracts.json"
csv_to_json(csvFilePath, jsonFilePath)
